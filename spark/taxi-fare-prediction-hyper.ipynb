{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c9cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6138f5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/09 17:09:45 WARN Utils: Your hostname, devkhk-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.30.1.27 instead (on interface en0)\n",
      "22/04/09 17:09:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/09 17:09:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# parquet 압축 코덱 선택, defalut:\"snappy\"\n",
    "MAX_MEMORY = \"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediction\")\\\n",
    "            .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "            .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "            .config(\"spark.sql.parquet.compression.codec\", None)\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ee03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *학습용 데이터를 꺼내 쓸 수 있도록 저장하기\n",
    "data_dir = \"/Users/devkhk/Documents/data-engineering-study/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d43b2ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 저장된 parquet 불러오기\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2e4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_df 학습용 데이터가 너무 크기 때문에 적당한 데이터를 sampling 한다.\n",
    "toy_df = train_df.sample(False, 0.1, seed=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c64a1d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()\n",
    "toy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e1c35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot Encoding : Wednesday -> 4 -> [0,0,0,1,0,0,0]\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "cat_feats = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "stages = []\n",
    "\n",
    "for c in cat_feats:\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol= c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c + \"_onehot\"])\n",
    "    stages += [cat_indexer, onehot_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f7f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_feats = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_feats:\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol= n + \"_vector\")\n",
    "    num_scalar = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol= n + \"_scaled\")\n",
    "    stages += [num_assembler, num_scalar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9107ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_input = [c + \"_onehot\" for c in cat_feats] + [n + \"_scaled\" for n in num_feats]\n",
    "assembler = VectorAssembler(inputCols=assembler_input, outputCol=\"feature_vector\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a3e5d",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "248897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(\n",
    "    maxIter=30,\n",
    "    solver=\"normal\",\n",
    "    labelCol='total_amount',\n",
    "    featuresCol='feature_vector'\n",
    ")\n",
    "\n",
    "cv_stages = stages + [lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505501bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pipeline = Pipeline(stages=cv_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c1de858",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder()\\\n",
    "                    .addGrid(lr.elasticNetParam, [0.1, 0.2, 0.3, 0.4, 0.5])\\\n",
    "                .addGrid(lr.regParam, [0.01, 0.02, 0.03, 0.04, 0.05])\\\n",
    "                .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec5abf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidator를 실행할 인스턴스 생성\n",
    "cross_val = CrossValidator(estimator=cv_pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"total_amount\"),\n",
    "                           numFolds=5\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "111876ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/09 18:22:28 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/04/09 18:22:28 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/04/09 18:22:30 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/04/09 18:22:30 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cv_model = cross_val.fit(toy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9542ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = cv_model.bestModel.stages[-1]._java_obj.getElasticNetParam()\n",
    "reg_param = cv_model.bestModel.stages[-1]._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80585e5",
   "metadata": {},
   "source": [
    "## Traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00bc7769",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transform_stages = stages\n",
    "pipeline = Pipeline(stages=transform_stages)\n",
    "fitted_transformer = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ee2fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "112999fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "        maxIter=50,\n",
    "        solver=\"normal\",\n",
    "        labelCol=\"total_amount\",\n",
    "        featuresCol=\"feature_vector\",\n",
    "        elasticNetParam=alpha,\n",
    "        regParam=reg_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c109371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = lr.fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92209f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtest_df = fitted_transformer.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b9b2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(vtest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a7eb66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, pickup_location_id: int, dropoff_location_id: int, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, passenger_count_vector: vector, passenger_count_scaled: vector, trip_distance_vector: vector, trip_distance_scaled: vector, pickup_time_vector: vector, pickup_time_scaled: vector, feature_vector: vector, prediction: double]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "828d5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 3046:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+------------------+\n",
      "|day_of_week|trip_distance|total_amount|        prediction|\n",
      "+-----------+-------------+------------+------------------+\n",
      "|   Thursday|          1.6|        12.3|14.905032148096899|\n",
      "|   Saturday|          3.3|       23.15|20.335917682834957|\n",
      "|  Wednesday|          4.1|        16.3| 16.11540300827887|\n",
      "|   Thursday|          0.4|         5.8| 8.136376385141844|\n",
      "|   Thursday|         15.4|        65.3| 47.26759901943578|\n",
      "|     Friday|          3.8|        13.3|  46.6271718097099|\n",
      "|     Friday|          4.6|        17.8|  48.1340994114685|\n",
      "|  Wednesday|         15.2|        76.3| 66.94334757308079|\n",
      "|     Sunday|          3.5|        17.3| 18.84901665167066|\n",
      "|     Monday|          6.3|        24.3|26.543356292003317|\n",
      "|   Saturday|          5.6|       27.35| 24.55224258636062|\n",
      "|    Tuesday|          7.6|       32.75|30.046050392502785|\n",
      "|  Wednesday|          0.1|         8.8| 12.37799176709074|\n",
      "|  Wednesday|          2.0|        12.8|16.751342623248892|\n",
      "|     Monday|          2.0|        15.8|16.368626347518443|\n",
      "|     Friday|          3.6|       20.75| 20.64708625351806|\n",
      "|   Saturday|          4.5|        20.3|22.293420211923724|\n",
      "|   Saturday|          3.5|       19.56| 19.06859748558028|\n",
      "|   Saturday|          4.9|        24.3| 23.13131594298647|\n",
      "|    Tuesday|          0.8|         8.3|13.204988549955665|\n",
      "+-----------+-------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select([\"day_of_week\",\"trip_distance\", \"total_amount\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d25a09f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.652536196100418"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d04e6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8082177057003137"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7254de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오랜시간 걸린 모델을 저장 & 로딩\n",
    "model_dir = \"/Users/devkhk/Documents/data-engineering-study/data/model\"\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "lr_model = LinearRegressionModel.load(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
