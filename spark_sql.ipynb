{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4d3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0baef9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 환경 설정\n",
    "conf = SparkConf().setMaster('local').setAppName(\"saprk_sql_study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf81d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/30 23:13:02 WARN Utils: Your hostname, devkhk-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.30.1.27 instead (on interface en0)\n",
      "22/03/30 23:13:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/30 23:13:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# spark Context 생성\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5dcacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.port', '49769'),\n",
       " ('spark.driver.host', '172.30.1.27'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1648649583948'),\n",
       " ('spark.app.startTime', '1648649582844'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'saprk_sql_study'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19e5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = sc.parallelize([\n",
    "    (1, (\"Google\", \"GOOGL\", \"USA\")),\n",
    "    (2, (\"Netflix\", \"NFLX\", \"USA\")),\n",
    "    (3, (\"Amazon\", \"AMZN\", \"USA\")),\n",
    "    (4, (\"Tesla\", \"TSLA\", \"USA\")),\n",
    "    (5, (\"Samsung\", \"005930\", \"Korea\")),\n",
    "    (6, (\"Kakao\", \"035720\", \"Korea\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8469e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = sc.parallelize([\n",
    "    (1, (2984, \"USA\")),\n",
    "    (2, (645, \"USA\")),\n",
    "    (3, (3518, \"USA\")),\n",
    "    (4, (1222, \"USA\")),\n",
    "    (5, (70600, \"USA\")),\n",
    "    (6, (125000, \"USA\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54088e6e",
   "metadata": {},
   "source": [
    "# 문제. 2000달러 이상의 미국 주식만 가져오기\n",
    "\n",
    "- 가능한 방법은 몇가지 일까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6ab14ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (('Google', 'GOOGL', 'USA'), (2984, 'USA'))),\n",
       " (3, (('Amazon', 'AMZN', 'USA'), (3518, 'USA')))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case1\n",
    "# inner join 이후 filter\n",
    "\n",
    "ticker_price = ticker.join(prices)\n",
    "ticker_price.filter(lambda x : x[1][0][2] == \"USA\" and x[1][1][0] >= 2000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a91ebf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (('Google', 'GOOGL', 'USA'), (2984, 'USA'))),\n",
       " (3, (('Amazon', 'AMZN', 'USA'), (3518, 'USA')))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/31 00:52:10 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1054869 ms exceeds timeout 120000 ms\n",
      "22/03/31 00:52:10 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# case2\n",
    "# filter 이후 join\n",
    "\n",
    "filter_ticker = ticker.filter(lambda x : x[1][2] == \"USA\")\n",
    "filter_prices = prices.filter(lambda x : x[1][0] >= 2000)\n",
    "\n",
    "filter_ticker.join(filter_prices).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ac5de",
   "metadata": {},
   "source": [
    "퍼포먼스는 case 2가 더 좋다.\n",
    "셔플링을 최소화 할 수 있기 때문\n",
    "\n",
    "매번 이런 고민을 해야한다면 개발자에 따라 코드에 따라 매번 성능이 들쑥 날쑥할것이다.\n",
    "\n",
    "하지만 구조화된 데이터들은 스파크에서 자동으로 최적화가 가능하게 해준다.\n",
    "\n",
    "## 구조화된 데이터의 종류\n",
    "### Unstructure  : free form\n",
    "- 로그파일\n",
    "- 이미지\n",
    "\n",
    "### Semi Structured : 행과 열\n",
    "- csv\n",
    "- json\n",
    "- xml\n",
    "\n",
    "### Structured : 행과 열 + 데이터 타입 (스키마)\n",
    "- 데이터베이스\n",
    "\n",
    "\n",
    "### Spark SQL\n",
    "Structured Data 에서는 데이터의 구조를 이미 알고 있으므로 어떤 테스를 수행할 것인지 정의만 하면된다.\n",
    "최적화를 자동으로 할 수 있음\n",
    "\n",
    "### Spark SQL 사용 목적\n",
    "- 스파크 프로그래밍 내부에서 관계형 처리를 하기 위해\n",
    "- 스키마의 정보를 이용해 자동으로 최적화를 하기 위해\n",
    "- 외부 데이터셋을 사용하기 쉽게 하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61ecf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
