{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7100733b",
   "metadata": {},
   "source": [
    "## 파이프 라인 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ffc3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd136dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 17:48:31 WARN Utils: Your hostname, devkhk-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.30.1.27 instead (on interface en0)\n",
      "22/04/12 17:48:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/12 17:48:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline-reivew\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fdea174",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01bdbe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6159090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b9e33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e315a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text 칼럼 토크나이저\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"word\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b3a8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "        maxIter=30,\n",
    "        regParam=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2005a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프 라인 만들기\n",
    "\n",
    "stages = [tokenizer, hashingTF, lr]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6410eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f8a282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "003ea668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- word: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f71f7e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+----------+--------------------+\n",
      "|            text|label|prediction|         probability|\n",
      "+----------------+-----+----------+--------------------+\n",
      "| a b c d e spark|  1.0|       1.0|[0.01762147996804...|\n",
      "|             b d|  0.0|       0.0|[0.97643765601314...|\n",
      "|     spark f g h|  1.0|       1.0|[0.01516075117785...|\n",
      "|hadoop mapreduce|  0.0|       0.0|[0.99078008726829...|\n",
      "+----------------+-----+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"text\",\"label\",\"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10961549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_e8bfe6d7457c"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b7136",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b55946",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778375a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261f59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "44421321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? \n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5a55181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable               Type                  Data/Info\n",
      "------------------------------------------------------\n",
      "HashingTF              ABCMeta               <class 'pyspark.ml.feature.HashingTF'>\n",
      "LogisticRegression     ABCMeta               <class 'pyspark.ml.classi<...>tion.LogisticRegression'>\n",
      "Pipeline               ABCMeta               <class 'pyspark.ml.pipeline.Pipeline'>\n",
      "SparkSession           type                  <class 'pyspark.sql.session.SparkSession'>\n",
      "Tokenizer              ABCMeta               <class 'pyspark.ml.feature.Tokenizer'>\n",
      "hashingTF              HashingTF             HashingTF_66a190f24e1b\n",
      "lr                     LogisticRegression    LogisticRegression_3903fc2a8ff0\n",
      "model                  PipelineModel         PipelineModel_409d96e7d41b\n",
      "pipeline               Pipeline              Pipeline_b2c861568757\n",
      "prediction             DataFrame             DataFrame[id: bigint, tex<...>ctor, prediction: double]\n",
      "spark                  SparkSession          <pyspark.sql.session.Spar<...>object at 0x7fdc1a4053d0>\n",
      "test                   DataFrame             DataFrame[id: bigint, text: string]\n",
      "token_hashing_stages   list                  n=3\n",
      "tokenizer              Tokenizer             Tokenizer_39b145091883\n",
      "training               DataFrame             DataFrame[id: bigint, tex<...>t: string, label: double]\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8afe2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a5c1ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('pipeline-review').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c8bd549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"l m j spark\", 1.0),\n",
    "    (4, \"tomoa c d t\", 0.0),\n",
    "    (5, \"hadoop mapreduce spark\", 1.0),\n",
    "    (6, \"m r l j\", 0.0),\n",
    "    (7, \"go python java\", 0.0),\n",
    "    (8, \"k t s k u spark\", 1.0),\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "126a03b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  0|     a b c d e spark|  1.0|\n",
      "|  1|                 b d|  0.0|\n",
      "|  2|         spark f g h|  1.0|\n",
      "|  3|         l m j spark|  1.0|\n",
      "|  4|         tomoa c d t|  0.0|\n",
      "|  5|hadoop mapreduce ...|  1.0|\n",
      "|  6|             m r l j|  0.0|\n",
      "|  7|      go python java|  0.0|\n",
      "|  8|     k t s k u spark|  1.0|\n",
      "+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c89d3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (9, \"spark i j k\"),\n",
    "    (10, \"l m n\"),\n",
    "    (11, \"spark hadoop spark\"),\n",
    "    (12, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f4a3422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6f686af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "af52d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7710f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "            maxIter=30,\n",
    "            regParam=0.3,\n",
    "            labelCol=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fc41cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_hashing_stages = [tokenizer, hashingTF, lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b468d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=token_hashing_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ad0c1f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "845c5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "89a856ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+----------+\n",
      "| id|              text|         probability|prediction|\n",
      "+---+------------------+--------------------+----------+\n",
      "|  9|       spark i j k|[0.25990624813819...|       1.0|\n",
      "| 10|             l m n|[0.56564059196989...|       0.0|\n",
      "| 11|spark hadoop spark|[0.08128966590605...|       1.0|\n",
      "| 12|     apache hadoop|[0.46878517783038...|       1.0|\n",
      "+---+------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(['id', 'text', 'probability', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c94aac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/Users/devkhk/Documents/data-engineering-study/data/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "25f37029",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3185bf",
   "metadata": {},
   "source": [
    "---\n",
    "## 불러온 모델 학습 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "cad7f53d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                  Type                  Data/Info\n",
      "---------------------------------------------------------\n",
      "HashingTF                 ABCMeta               <class 'pyspark.ml.feature.HashingTF'>\n",
      "LogisticRegression        ABCMeta               <class 'pyspark.ml.classi<...>tion.LogisticRegression'>\n",
      "LogisticRegressionModel   ABCMeta               <class 'pyspark.ml.classi<...>LogisticRegressionModel'>\n",
      "Pipeline                  ABCMeta               <class 'pyspark.ml.pipeline.Pipeline'>\n",
      "PipelineModel             ABCMeta               <class 'pyspark.ml.pipeline.PipelineModel'>\n",
      "SparkSession              type                  <class 'pyspark.sql.session.SparkSession'>\n",
      "Tokenizer                 ABCMeta               <class 'pyspark.ml.feature.Tokenizer'>\n",
      "hashingTF                 HashingTF             HashingTF_54f111febde5\n",
      "load_model                PipelineModel         PipelineModel_ef71787ff92a\n",
      "lr                        LogisticRegression    LogisticRegression_5d19e87d4e72\n",
      "model                     PipelineModel         PipelineModel_ef71787ff92a\n",
      "model_dir                 str                   /Users/devkhk/Documents/d<...>ineering-study/data/model\n",
      "pipeline                  Pipeline              Pipeline_d2f33710c995\n",
      "prediction                DataFrame             DataFrame[id: bigint, tex<...>ctor, prediction: double]\n",
      "spark                     SparkSession          <pyspark.sql.session.Spar<...>object at 0x7fdc1a4053d0>\n",
      "test                      DataFrame             DataFrame[id: bigint, text: string]\n",
      "token_hashing_stages      list                  n=3\n",
      "tokenizer                 Tokenizer             Tokenizer_5cdc6c967943\n",
      "training                  DataFrame             DataFrame[id: bigint, tex<...>t: string, label: double]\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "79f19d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f641e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "04b1c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e673ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/Users/devkhk/Documents/data-engineering-study/data/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f7b65108",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = PipelineModel.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f80880cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('load-pipeline-model').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5aaf4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (9, \"spark i j k\"),\n",
    "    (10, \"l m n\"),\n",
    "    (11, \"spark hadoop spark\"),\n",
    "    (12, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a02dc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = load_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "846eada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c3f2c860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+--------------------+\n",
      "|              text|prediction|         probability|\n",
      "+------------------+----------+--------------------+\n",
      "|       spark i j k|       1.0|[0.25990624813819...|\n",
      "|             l m n|       0.0|[0.56564059196989...|\n",
      "|spark hadoop spark|       1.0|[0.08128966590605...|\n",
      "|     apache hadoop|       1.0|[0.46878517783038...|\n",
      "+------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(\"text\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8048ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
