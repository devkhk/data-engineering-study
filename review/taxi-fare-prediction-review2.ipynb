{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10379e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c27f9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: spakr.driver.memory\n",
      "22/04/20 16:41:38 WARN Utils: Your hostname, devkhk-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.30.1.27 instead (on interface en0)\n",
      "22/04/20 16:41:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/20 16:41:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY = \"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediction\")\\\n",
    "                            .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "                            .config(\"spakr.driver.memory\", MAX_MEMORY)\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c2692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50785d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 데이터 담긴 디렉토리 파일\n",
    "trips_dir = \"/Users/devkhk/Documents/data-engineering-study/data/trips/*\"\n",
    "trips_df = spark.read.csv(f\"file:///{taxi_dir}\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fae1089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d94c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d372dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    trip_distance,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 4\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\n",
    "\"\"\"\n",
    "comb_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29422323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|     trip_distance|      total_amount|\n",
      "+-------+------------------+------------------+\n",
      "|  count|          13126040|          13126040|\n",
      "|   mean|2.8820930920520915|17.973158757890285|\n",
      "| stddev| 3.820306480671185|12.975904680786682|\n",
      "|    min|              0.01|              0.01|\n",
      "|    max|             475.5|            4973.3|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "comb_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad7a9223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|trip_distance|total_amount|\n",
      "+-------------+------------+\n",
      "|         16.5|       70.07|\n",
      "|         1.13|       11.16|\n",
      "|         2.68|       18.59|\n",
      "|         12.4|        43.8|\n",
      "|          9.7|        32.3|\n",
      "|          9.3|       43.67|\n",
      "|         9.58|        46.1|\n",
      "|         16.2|        45.3|\n",
      "|         3.58|        19.3|\n",
      "|         0.91|        14.8|\n",
      "|         2.57|        12.8|\n",
      "|          0.4|         5.3|\n",
      "|         3.26|        17.3|\n",
      "|        13.41|       47.25|\n",
      "|         18.3|       61.42|\n",
      "|         1.53|       14.16|\n",
      "|          2.0|        11.8|\n",
      "|         16.6|       54.96|\n",
      "|         15.5|       56.25|\n",
      "|          1.3|        16.8|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59a2be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip_distance 를 벡터로 변경하기\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vassembler = VectorAssembler(inputCols=[\"trip_distance\"], outputCol=\"features\")\n",
    "\n",
    "vcomb_df = vassembler.transform(comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e19e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------+\n",
      "|trip_distance|total_amount|features|\n",
      "+-------------+------------+--------+\n",
      "|         16.5|       70.07|  [16.5]|\n",
      "|         1.13|       11.16|  [1.13]|\n",
      "|         2.68|       18.59|  [2.68]|\n",
      "|         12.4|        43.8|  [12.4]|\n",
      "|          9.7|        32.3|   [9.7]|\n",
      "|          9.3|       43.67|   [9.3]|\n",
      "|         9.58|        46.1|  [9.58]|\n",
      "|         16.2|        45.3|  [16.2]|\n",
      "|         3.58|        19.3|  [3.58]|\n",
      "|         0.91|        14.8|  [0.91]|\n",
      "|         2.57|        12.8|  [2.57]|\n",
      "|          0.4|         5.3|   [0.4]|\n",
      "|         3.26|        17.3|  [3.26]|\n",
      "|        13.41|       47.25| [13.41]|\n",
      "|         18.3|       61.42|  [18.3]|\n",
      "|         1.53|       14.16|  [1.53]|\n",
      "|          2.0|        11.8|   [2.0]|\n",
      "|         16.6|       54.96|  [16.6]|\n",
      "|         15.5|       56.25|  [15.5]|\n",
      "|          1.3|        16.8|   [1.3]|\n",
      "+-------------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vcomb_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "785fa6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용, 테스트용 데이터 분할\n",
    "train_df, test_df = vcomb_df.randomSplit([.8, .2], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed2a9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "193e7f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "        maxIter=30,\n",
    "        labelCol=\"total_amount\",\n",
    "        regParam=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aec8bd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/20 17:26:22 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/04/20 17:26:22 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/04/20 17:26:31 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ab71cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de1e5d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[trip_distance: double, total_amount: double, features: vector, prediction: double]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9bad7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------+-----------------+\n",
      "|trip_distance|total_amount|features|       prediction|\n",
      "+-------------+------------+--------+-----------------+\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.3|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.8|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.8|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.8|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.8|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.8|  [0.01]|9.437003859544042|\n",
      "|         0.01|         3.8|  [0.01]|9.437003859544042|\n",
      "+-------------+------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68a2fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = [5.0, 15.0, 20.0, 25.5]\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "distance_df = spark.createDataFrame(distance, FloatType()).toDF(\"trip_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cca616f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdistance_df = vassembler.transform(distance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e34f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_predictions = model.transform(vdistance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b9a82ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------------+\n",
      "|trip_distance|features|        prediction|\n",
      "+-------------+--------+------------------+\n",
      "|          5.0|   [5.0]| 24.26910638151479|\n",
      "|         15.0|  [15.0]|53.992758730153554|\n",
      "|         20.0|  [20.0]| 68.85458490447293|\n",
      "|         25.5|  [25.5]| 85.20259369622426|\n",
      "+-------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7625d0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7648629263876808"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf9170fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.307820185476748"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "883aed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스크를 넣으면 결과를 예측해 주는 함수로 만들어보기\n",
    "def taxi_fare_predict(dl):\n",
    "    dl = list(map(lambda x:float(x), dl))\n",
    "    from pyspark.sql.types import FloatType\n",
    "    df = spark.createDataFrame(dl, FloatType()).toDF(\"trip_distance\")\n",
    "    \n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    vassembler = VectorAssembler(inputCols=[\"trip_distance\"], outputCol=\"features\")\n",
    "    vdf = vassembler.transform(df)\n",
    "    \n",
    "    # 모델 불러오기 (생략)\n",
    "    \n",
    "    # 예측\n",
    "    predictions = model.transform(vdf)\n",
    "    \n",
    "    return predictions.select([\"trip_distance\", \"prediction\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f42be0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|trip_distance|        prediction|\n",
      "+-------------+------------------+\n",
      "|          1.0|12.379645442059282|\n",
      "|          2.0|15.352010676923157|\n",
      "|          3.0|18.324375911787037|\n",
      "|          4.0| 21.29674114665091|\n",
      "|          4.1| 21.59397738667046|\n",
      "|         22.5| 76.28549799163262|\n",
      "|         30.2| 99.17271256781918|\n",
      "|         57.9| 181.5072318412833|\n",
      "|         58.0|181.80446382930026|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_fare_predict([1, 2, 3, 4,4.1, 22.5, 30.2, 57.9, 58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ae8bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418e0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
