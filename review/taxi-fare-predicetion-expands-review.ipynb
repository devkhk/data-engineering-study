{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c65f07",
   "metadata": {},
   "source": [
    "## 예측 모델 정확도 올리기 위해 다른 변수들을 추가해서 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ca4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb233f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 13:19:47 WARN Utils: Your hostname, devkhk-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.30.1.27 instead (on interface en0)\n",
      "22/04/21 13:19:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/21 13:19:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY = \"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediction-expands\")\\\n",
    "                            .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "                            .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cddd66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_dir = \"/Users/devkhk/Documents/data-engineering-study/data/trips/*\"\n",
    "trips_df = spark.read.csv(f\"file:///{trips_dir}\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb96afbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c68a6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1260dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as hour,\n",
    "    DAYOFWEEK(TO_DATE(tpep_pickup_datetime)) as day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 4\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\"\n",
    "\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4732a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|hour|day_of_week|total_amount|\n",
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "|              0|               138|                265|         16.5|   0|          2|       70.07|\n",
      "|              1|                68|                264|         1.13|   0|          2|       11.16|\n",
      "|              1|               239|                262|         2.68|   0|          2|       18.59|\n",
      "|              1|               186|                 91|         12.4|   0|          2|        43.8|\n",
      "|              2|               132|                265|          9.7|   0|          2|        32.3|\n",
      "|              1|               138|                141|          9.3|   0|          2|       43.67|\n",
      "|              1|               138|                 50|         9.58|   0|          2|        46.1|\n",
      "|              1|               132|                123|         16.2|   0|          2|        45.3|\n",
      "|              1|               140|                  7|         3.58|   0|          2|        19.3|\n",
      "|              1|               239|                238|         0.91|   0|          2|        14.8|\n",
      "|              2|               116|                 41|         2.57|   0|          2|        12.8|\n",
      "|              1|                74|                 41|          0.4|   0|          2|         5.3|\n",
      "|              1|               239|                144|         3.26|   0|          2|        17.3|\n",
      "|              1|               132|                 91|        13.41|   0|          2|       47.25|\n",
      "|              2|               132|                230|         18.3|   0|          2|       61.42|\n",
      "|              1|               229|                 48|         1.53|   0|          2|       14.16|\n",
      "|              1|                48|                 68|          2.0|   0|          2|        11.8|\n",
      "|              2|               132|                255|         16.6|   0|          2|       54.96|\n",
      "|              1|               132|                145|         15.5|   0|          2|       56.25|\n",
      "|              2|                79|                164|          1.3|   0|          2|        16.8|\n",
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e0d5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([.8, .2], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c3428d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# parquet형태 저장\n",
    "data_dir = \"/Users/devkhk/Documents/data-engineering-study/data/\"\n",
    "train_df.write.parquet(path=f\"{data_dir}train-review/\",mode=\"overwrite\")\n",
    "test_df.write.parquet(f\"{data_dir}test-review/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f2ef118",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d190106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable       Type            Data/Info\n",
      "----------------------------------------\n",
      "MAX_MEMORY     str             5g\n",
      "SparkSession   type            <class 'pyspark.sql.session.SparkSession'>\n",
      "data_df        DataFrame       DataFrame[passenger_count<...>nt, total_amount: double]\n",
      "data_dir       str             /Users/devkhk/Documents/d<...>a-engineering-study/data/\n",
      "query          str             \\nSELECT\\n    passenger_c<...>atetime) < '2021-08-01'\\n\n",
      "spark          SparkSession    <pyspark.sql.session.Spar<...>object at 0x7ff2027e58b0>\n",
      "trips_df       DataFrame       DataFrame[VendorID: int, <...>estion_surcharge: double]\n",
      "trips_dir      str             /Users/devkhk/Documents/d<...>eering-study/data/trips/*\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8892732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parqeut 불러오기\n",
    "train_df = spark.read.parquet(f\"file:///{data_dir}train-review/\")\n",
    "test_df = spark.read.parquet(f\"file:///{data_dir}test-review/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e869fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|hour|day_of_week|total_amount|\n",
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "|              0|                 4|                  4|          0.1|   2|          1|         6.8|\n",
      "|              0|                 4|                  4|          2.2|   2|          7|        15.3|\n",
      "|              0|                 4|                 48|          2.8|  16|          7|        19.3|\n",
      "|              0|                 4|                 79|          0.6|  14|          5|         8.3|\n",
      "|              0|                 4|                 87|          2.7|  15|          6|        15.8|\n",
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|hour|day_of_week|total_amount|\n",
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "|              0|                 4|                107|          1.6|  20|          5|        12.3|\n",
      "|              0|                 4|                256|          3.3|  23|          7|       23.15|\n",
      "|              0|                 7|                 70|          4.1|  16|          4|        16.3|\n",
      "|              0|                 7|                193|          0.4|  16|          5|         5.8|\n",
      "|              0|                 9|                  9|         15.4|  20|          5|        65.3|\n",
      "+---------------+------------------+-------------------+-------------+----+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bbda200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2e5176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51e9a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 / Numberic Feature 분류 => pipeline stages 만들기\n",
    "stages = []\n",
    "\n",
    "cat_features = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"hour\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "num_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol= c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "    one_encoder = OneHotEncoder(inputCol=cat_indexer.getOutputCol(), outputCol= c + \"_onehot\")\n",
    "    stages += [cat_indexer, one_encoder]\n",
    "\n",
    "for n in num_features:\n",
    "    num_vector = VectorAssembler(inputCols=[n], outputCol= n + \"_vector\")\n",
    "    num_std = StandardScaler(inputCol=num_vector.getOutputCol(), outputCol=n + \"_std\")\n",
    "    stages += [num_vector, num_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e461a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_fc2486a1ca25,\n",
       " OneHotEncoder_cc5790cc8a13,\n",
       " StringIndexer_6e9b789b994f,\n",
       " OneHotEncoder_799eccab5615,\n",
       " StringIndexer_b78e0c5ab2d4,\n",
       " OneHotEncoder_97e28f3f9de4,\n",
       " StringIndexer_96bb03d0147f,\n",
       " OneHotEncoder_af4a1ee7ca13,\n",
       " VectorAssembler_d515f97b3d18,\n",
       " StandardScaler_7f68c742c379,\n",
       " VectorAssembler_bee950cfefe4,\n",
       " StandardScaler_16f1b098f881]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19aa0100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickup_location_id_onehot',\n",
       " 'dropoff_location_id_onehot',\n",
       " 'hour_onehot',\n",
       " 'day_of_week_onehot',\n",
       " 'passenger_count_std',\n",
       " 'trip_distance_std']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler = [c+\"_onehot\" for c in cat_features] + [n + \"_std\" for n in num_features]\n",
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97bc93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vassembler = VectorAssembler(inputCols=assembler, outputCol=\"features\")\n",
    "stages += [vassembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a50f2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_fc2486a1ca25,\n",
       " OneHotEncoder_cc5790cc8a13,\n",
       " StringIndexer_6e9b789b994f,\n",
       " OneHotEncoder_799eccab5615,\n",
       " StringIndexer_b78e0c5ab2d4,\n",
       " OneHotEncoder_97e28f3f9de4,\n",
       " StringIndexer_96bb03d0147f,\n",
       " OneHotEncoder_af4a1ee7ca13,\n",
       " VectorAssembler_d515f97b3d18,\n",
       " StandardScaler_7f68c742c379,\n",
       " VectorAssembler_bee950cfefe4,\n",
       " StandardScaler_16f1b098f881,\n",
       " VectorAssembler_94ebbaede811]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b010fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fcc64502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitted_transformer = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4a0e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b49abae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(556,[62,311,543,...|\n",
      "|(556,[62,311,543,...|\n",
      "|(556,[62,273,527,...|\n",
      "|(556,[62,280,526,...|\n",
      "|(556,[62,308,524,...|\n",
      "|(556,[62,290,540,...|\n",
      "|(556,[62,279,529,...|\n",
      "|(556,[62,299,534,...|\n",
      "|(556,[62,288,528,...|\n",
      "|(556,[62,310,531,...|\n",
      "|(556,[62,303,532,...|\n",
      "|(556,[62,303,543,...|\n",
      "|(556,[62,301,523,...|\n",
      "|(556,[62,301,533,...|\n",
      "|(556,[62,301,527,...|\n",
      "|(556,[62,282,533,...|\n",
      "|(556,[62,266,536,...|\n",
      "|(556,[62,266,528,...|\n",
      "|(556,[62,370,525,...|\n",
      "|(556,[62,324,526,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vtrain_df.select([\"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "812c9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataFrame 을 vector로 만드는 파이프라인을 통과 시켰으니 모델 학습을 추가한다.\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(\n",
    "    maxIter=30,\n",
    "    regParam=0.01,\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53fd2399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 16:04:50 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/04/21 16:04:50 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/04/21 16:05:00 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# vtrain_df를 모델 학습\n",
    "model = lr.fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc36fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtest_df = fitted_transformer.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0110edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(vtest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6f9e7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- hour_idx: double (nullable = false)\n",
      " |-- hour_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vector: vector (nullable = true)\n",
      " |-- passenger_count_std: vector (nullable = true)\n",
      " |-- trip_distance_vector: vector (nullable = true)\n",
      " |-- trip_distance_std: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c65a967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------------+------------------+\n",
      "|day_of_week|hour|total_amount|        prediction|\n",
      "+-----------+----+------------+------------------+\n",
      "|          5|  20|        12.3|14.635430095883565|\n",
      "|          7|  23|       23.15|19.958199770493934|\n",
      "|          4|  16|        16.3|16.487046381443452|\n",
      "|          5|  16|         5.8| 8.741823421141877|\n",
      "|          5|  20|        65.3|45.438651128967834|\n",
      "|          6|  13|        13.3|47.755725589684076|\n",
      "|          6|   2|        17.8|49.465441940257406|\n",
      "|          4|  17|        76.3| 68.78800790045243|\n",
      "|          1|  17|        17.3| 20.00459903495035|\n",
      "|          2|  17|        24.3| 27.73866726998494|\n",
      "|          7|  16|       27.35| 26.02304078097364|\n",
      "|          3|  19|       32.75|30.188940913571383|\n",
      "|          4|  10|         8.8|12.376017430698168|\n",
      "|          4|  14|        12.8|17.114979497261892|\n",
      "|          2|  16|        15.8| 17.69950086110069|\n",
      "|          6|  19|       20.75|20.560003900267276|\n",
      "|          7|  19|        20.3|22.117623882696684|\n",
      "|          7|   6|       19.56| 17.83064055789398|\n",
      "|          7|  11|        24.3|23.042369727332456|\n",
      "|          3|   9|         8.3|13.341033794911912|\n",
      "+-----------+----+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select([\"day_of_week\", \"hour\", \"total_amount\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a28308f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.622837560755986"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b0cb700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8102252707480382"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28212fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
